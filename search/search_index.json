{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>I'm a AI Engineer at Faktion, specialized in Generative AI evaluation methodologies. </p> <p>My work involves designing AI Soltuions, architecting LLM agents and implementing structured evaluation processes for AI Assistants.</p> <p> Connect on LinkedIn Download My CV</p>","tags":["Machine Learning","Generative AI","AI Evaluation","LLM Implementation","Open Source"]},{"location":"#lets-connect","title":"Let's Connect","text":"<p>I'm always excited to discuss AI evaluation methodologies, share insights about LLM implementation challenges, or explore potential collaborations. Whether you're looking to improve your AI systems or discuss the latest in generative AI evaluation, I'd love to connect.</p> <p> Book a Meeting</p>","tags":["Machine Learning","Generative AI","AI Evaluation","LLM Implementation","Open Source"]},{"location":"#featured-work-insights","title":"Featured Work &amp; Insights","text":"<p>I write extensively about LLM evaluation, enterprise AI adoption challenges, and practical methodologies for building reliable AI systems. Here are some key insights from my work:</p>","tags":["Machine Learning","Generative AI","AI Evaluation","LLM Implementation","Open Source"]},{"location":"#core-expertise","title":"Core Expertise","text":"<ul> <li>LLM Evaluation Pipelines: Developing systematic approaches to measure and improve generative AI performance</li> <li>Evaluation-Driven Development: Creating methodologies that bridge the gap between automated metrics and user satisfaction</li> <li>Enterprise AI Adoption: Understanding why evaluation, not models, is often the limiting factor in AI deployment</li> </ul>","tags":["Machine Learning","Generative AI","AI Evaluation","LLM Implementation","Open Source"]},{"location":"#key-insights-publications","title":"Key Insights &amp; Publications","text":"<p>Learn about my evaluation methodologies View all writing</p>","tags":["Machine Learning","Generative AI","AI Evaluation","LLM Implementation","Open Source"]},{"location":"#open-source-contributions","title":"Open Source Contributions","text":"<p>I'm also one of the creators of YAKE, a popular keyword extraction library with over 1.7k stars. YAKE! is a light-weight unsupervised automatic keyword extraction method that relies on text statistical features to select the most important keywords from single documents.</p>","tags":["Machine Learning","Generative AI","AI Evaluation","LLM Implementation","Open Source"]},{"location":"#academic-involvement","title":"Academic Involvement","text":"<p>Reviewer in academic conferences and workshops: </p> <ul> <li>ECML PKDD 2025 - European Conference on Machine Learning</li> <li>International Workshop on Narrative Extraction from Texts (2018 - 2025)</li> <li>ECIR 2020 - European Conference on Information Retrieval</li> <li>International Workshop on User Interfaces for Spatial and Temporal Data Analysis (2019) </li> </ul> <p> Google Scholar Profile</p>","tags":["Machine Learning","Generative AI","AI Evaluation","LLM Implementation","Open Source"]},{"location":"404/","title":"404 - Page not found","text":"<p>Sorry, the page you were looking for doesn't exist.</p>","tags":["404 error","page not found","website navigation","broken links","user support"]},{"location":"404/#what-might-have-happened","title":"What might have happened?","text":"<ul> <li>The page might have been moved or deleted</li> <li>You might have followed a broken link</li> <li>There might be a typo in the URL</li> </ul>","tags":["404 error","page not found","website navigation","broken links","user support"]},{"location":"404/#what-can-you-do-now","title":"What can you do now?","text":"<ul> <li>Go back to the homepage</li> <li>Use the search function at the top of the page</li> </ul>","tags":["404 error","page not found","website navigation","broken links","user support"]},{"location":"writing/","title":"Writing","text":"<p>I write about a mix of consulting, personal work, and applying llms.</p> <p> Follow me on LinkedIn </p> <p>For posts about RAG (Retrieval-Augmented Generation) or LLMs (Large Language Models), check out the category labels in the sidebar. Here are some of my best posts on these topics:</p>","tags":["LLMs","AI evaluation","consulting","personal growth","RAG"]},{"location":"writing/#insights","title":"Insights","text":"<ul> <li>When LLM-as-Judge Metrics and User Preferences Diverge - My journey developing an evaluation-driven approach for generative AI systems, sharing lessons learned about structured feedback, multi-level evaluation, and building more reliable AI applications.</li> <li>LLMs as Judges: Why Automated Metrics Aren't Enough - My thoughts on the limitations of LLM-as-judge approaches</li> <li>The Real Bottleneck in Enterprise AI Adoption - Why evaluation, not models, is the limiting factor</li> <li>How Many Samples Do We Need in Our Evaluation Dataset? - Practical guidance on statistical confidence and sample sizes</li> <li>From Prompt to Context Engineering - Why evaluation is becoming the key differentiator</li> </ul>","tags":["LLMs","AI evaluation","consulting","personal growth","RAG"]},{"location":"writing/2025/04/20/enterprise-ai-adoption-bottleneck/","title":"The Real Bottleneck in Enterprise AI Adoption: Evaluation, Not Models","text":"<p>Enterprise AI adoption is moving much slower than the hype would suggest, and there\u2019s a clear reason: domain-specific evaluation. Public benchmarks and leaderboard scores mean little when it comes to real-world business needs. What matters is whether these systems can be trusted to perform reliably in your unique context\u2014and that requires custom evaluation pipelines and validation from domain experts.</p> <p>The real bottleneck isn\u2019t model capability or even energy consumption (at least, not yet). It\u2019s the scarcity of human expertise available to rigorously validate these systems. Without that, trust is impossible.</p> <p>It\u2019s tempting to think that every new model release from OpenAI or another big lab will instantly unlock new business value. But the reality is, just because a new reasoning model drops doesn\u2019t mean it will suddenly understand your business logic. Enterprise software isn\u2019t built on vibes\u2014it\u2019s built on structure, reliability, and determinism.</p> <p>Building production-grade, reasoning-based AI systems is hard. Getting agents to do real, valuable work is even harder. Most companies are still wrestling with the basics\u2014like generating a clean summary from a SharePoint folder. And that\u2019s okay. That\u2019s where the real work begins.</p> <p>Despite the noise from AI opportunists selling quick fixes, the truth is that almost no one has cracked operational AI at scale. Real credibility comes not from clever hacks, but from putting in the work: shipping infrastructure that quietly, reliably, and deeply integrates into the fabric of a company.</p> <p>AI in operations isn\u2019t a magic prompt. It\u2019s a ladder you climb, step by step. Using ChatGPT at work is just the entry point.</p> <p>Here\u2019s what the real AI adoption stack looks like:</p> Level Description Use Cases Skills Org Needs Level 1: Generative AI + Proprietary Data This is ChatGPT, but on your own documents. Writing reports, summarizing docs, answering FAQs Prompting, metadata management, content accuracy Basic data hygiene, light governance Level 2: Contextual AI + Knowledge Integration Now the model can access and use internal data automatically (think RAG). LLMs pulling in internal data Data pipelines, embeddings, retrieval tuning Strong taxonomy, content architecture, access controls Level 3: AI Agents for Business Tasks LLMs don\u2019t just talk\u2014they act. Agents processing tickets, scheduling meetings, writing emails API integration, reasoning and acting (ReAct prompting), tool orchestration Clear processes, oversight, evaluation Level 4: Multi-Agent Workflow Orchestration Agents coordinate with each other to automate entire workflows. Specialized agents collaborating and adapting Multi-agent architecture, AI-Ops, fallback design High AI maturity, observability, risk controls <p>Most teams today are somewhere between Level 1 and Level 2, using AI as a productivity boost rather than as a true system backbone. If you\u2019re eager to deploy agents but haven\u2019t yet mastered generating meaningful reports, you\u2019re skipping crucial steps.</p> <p>There are no shortcuts to maturity. No prompt hack or agent will build the necessary infrastructure for you. True operational AI isn\u2019t about looking clever\u2014it\u2019s about building systems that work, even when no one\u2019s watching.</p>","tags":["AI adoption","enterprise AI","model evaluation","business intelligence","AI development"]},{"location":"writing/2025/03/30/evaluation-dataset-sample-size/","title":"How many samples do we need in our evaluation dataset?","text":"<p>The most common question I hear when teams set up their evaluation framework: \"How many samples do we need?\" </p> <p>This question also comes in different forms , like  \"How much time do we need from our domain experts to annotate the dataset?\"</p> <p>In the context of big corporate business, domain experts are usually the most expensive resource in the company. Being able to get one hour of their time is serious money. So we need to prove how much exactly we need to borrow from their time to annotate our brand new AI project. </p> <p>The answer?  It depends on how much confidence you want in your results.</p> <p>Statistical significance matters and it matters even more in the context of evaluating generative AI systems. If you're making project decisions based on these evaluations, you need to know they're reliable.</p> <p>Here's how we frame this question. Let\u2019s consider the confidence level and margin of error we are comfortable with. 3 points to decide:</p> <ul> <li>The confidence level you require (90%, 95%, 99%)</li> <li>The margin of error you can accept</li> <li>The expected variance in your data</li> </ul> <p>For example: - For 90% confidence with \u00b15% margin of error: ~270 samples - For 95% confidence with \u00b15% margin of error: ~385 samples - For 99% confidence with \u00b15% margin of error: ~665 samples</p> <p>These numbers can be calculated using standard statistical formulas for sample size determination.</p> <p>Remember: Insufficient samples can lead to misleading results and poor decisions about your AI project. Invest in proper evaluation to build trust in your models and your decision-making process.</p> <p>Even better, quantify how much it will cost in annotation time versus the level of confidence you want to achieve.</p> <p></p>","tags":["AI evaluation","sample size","evaluation framework","confidence level","data annotation"]},{"location":"writing/2025/05/26/generative-ai-and-digital-humanities/","title":"Why Qualitative Research Experts Are the Secret Weapon for Generative AI Evaluation","text":"<p>To my colleagues in digital humanities, anthropology, and experts in qualitative research: you're sitting on a goldmine and don't even know it.</p> <p>While everyone's trying to learn \"prompt engineering,\" you are the true experts because you have been doing this for your entire career. </p> <p>You know how to write precise, nuanced questions that extract meaningful responses. </p> <p>You understand context, subtext, and how framing shapes answers. You're experts in context analysis, at iterative inquiry and grounded theory .</p> <p>You evaluate sources critically, find biases and inconsistencies, and know that the most interesting insights often lie in what's not being said. </p> <p>You understand that interpretation requires both rigor and creativity. </p> <p>These aren't just transferable skills, they're the skills that matter most when designing, evaluating and aligning LLMs or AI agents.</p> <p>The AI world is busy teaching engineers to think like qualitative researchers. </p> <p>Meanwhile, you already think like AI researchers. You just need to learn the syntax. When this clicks for our field, we won't just be catching up, we'll be leading the conversation on responsible AI evaluation and human-centered AI systems design.</p> <p>The skills to evaluate AI agents are not that different from evaluating humans. </p> <p>As I've specialized more in AI agent evaluation, I can't stop thinking about how my colleagues and I, as engineers, are 'discovering' methodologies that you have known for ages.</p> <p>You are so well equipped to help design evaluation or fine-tuning datasets and you don't even know it.</p> <p>The future of AI isn't just engineering. It is more human than ever. And that's your territory.</p>","tags":["Qualitative Research","Generative AI","AI Evaluation","Human-Centered Design","Digital Humanities"]},{"location":"writing/2025/04/17/generative-ai-protocols-and-tartare/","title":"From Prompt to Context Engineering: Why Evaluation is the Real AI Differentiator","text":"<p>Yesterday I had a fantastic dinner in London with Samuel Colvin the mind behind Pydantic (one of the most ubiquitous Python frameworks) and now focusing on PydanticAI and Logfire, Laura Modiano Strategic Partnerships Developer in Europe at OpenAI, David Soria Parra - Anthropic engineer behind the Model Context Protocol - Joon-sang Lee CEO of Pentaform, and industry legends Jason Liu and Ivan Leo - creators of instructor and Andreas Stathopoulos.</p> <p></p> <p>I had a realization: everyone building generative AI agents and assistants is writing remarkably similar code. We're all building redundant data plumbing systems to manage context from data sources to agents, backends, and UIs.</p> <p>Enought of AI data plumbing. The future and what will make or break your AI agents engineering is domain-specific evaluation.</p> <p>Let\u2019s acknowledge that we are at the infancy of this tech. As we get more mature, solutions like Anthropic's Model Context Protocol (MCP) are standardizing how we manage context and communicate AI with data sources more efficiently. If you are living under a rock and don\u2019t know MCP please check here. </p> <p>We're evolving beyond prompt engineering concatenating strings to become what I like to call \u201ccontext engineers\u201d. Our primary job is finding the most relevant context to place in front of LLMs to answer user needs.</p> <p>This context engineering for AI agents is fundamentally repetitive across domains. We're all: - Deriving user questions into research plans using chain-of-thought reasoning - Searching for relevant context - Synthesizing answers</p> <p>What struck me is where applied AI engineers should actually be focusing their time: building effective domain-specific evaluation workflows.</p> <p>As I discussed in my previous posts, the AI adoption bottleneck for enterprise is not model capabilities, is the ability to prove (with statistical rigor) the accuracy of these systems to the specific domain. </p> <p>I have been expending most of my time building evaluation pipelines for user validation and it is hard. Not everyone has the luxury of having millions of users to AB test. At the enterprise level, the time you can borrow from domain experts for validation is scarse.</p> <p>We need to shift our energy from repetitive data plumbing to building robust evaluation workflows and feedback loops. We should be assessing our systems' confidence when handling knowledge-specific topics and measuring accuracy with rigor.</p> <p>This is where I want to allocate the majority of project budgets going forward. With MCP's exponential adoption across the industry, the plumbing problem is being solved. The differentiator will be how well we can evaluate and improve our systems in specific domains.</p> <p>What are your thoughts? Are you seeing this shift in your AI projects? </p> <p>Also, I highly recommend checking PydanticAI  And the AI Observality platform Logfire</p>","tags":["Generative AI","Context Engineering","Domain-Specific Evaluation","AI Development","AI Models"]},{"location":"writing/2025/04/21/llms-as-judges-and-user-preference-correlation/","title":"When LLM-as-Judge Metrics and User Preferences Diverge: Lessons from Real-World Evaluation","text":"<p>Why you should deeply understand what your LLM-as-a-Judge metric is actually measuring</p> <p>When our automated evaluation metrics showed only 55% correctness for our LLM-generated answers, but users consistently preferred our system over 70% of the time, we knew something was off.</p> <p>After deep analysis , we found that our Ragas-inspired correctness metric was actually penalizing our system for being 'too informative'. The metric counts additional facts beyond the ground truth as 'False Positives' - effectively punishing more comprehensive answers.</p> <p>We changed the focus for another metric that according to our AB testing together with correlation analysis provided a more accurate picture.</p> <p>This metric now categorises answers as: - Subsets of expert validated answers (consistent but less comprehensive) - Supersets of expert validated answers (consistent with additional information) - Fully consistent/equivalent to expert validated answers - In disagreement with expert validated answers</p> <p>Using this approach, our actual factual accuracy jumps to 78%, much closer to what our user preference AB tests suggested.</p> <p>Key takeways</p> <ol> <li> <p>\ud835\uddd5\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\uddff\ud835\uddf2\ud835\uddf3\ud835\ude02\ud835\uddf9 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\udddf\ud835\udddf\ud835\udde0-\ud835\uddee\ud835\ude00-\ud835\uddee-\ud835\udddd\ud835\ude02\ud835\uddf1\ud835\uddf4\ud835\uddf2 \ud835\uddfa\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf0\ud835\ude00: they may not align with what users actually value. Always calibrate your metrics with AB testing results. What the users prefer is more important than any LLM judge. </p> </li> <li> <p>\ud835\uddd4\ud835\uddf9\ud835\ude04\ud835\uddee\ud835\ude06\ud835\ude00 \ud835\ude03\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddf3\ud835\ude06 \ud835\uddf0\ud835\uddfc\ud835\uddff\ud835\uddff\ud835\uddf2\ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb between automated LLM metrics and user preferences through AB testing</p> </li> </ol> <p>Finally, the most accurate metric isn't always the most complex one. It's the one that best predicts user satisfaction. I cannot stress enough how you should focus on AB tests for that. </p> <p>Proper user AB testing is more important than any LLM-as-a-Judge metric.</p> <p>Revisit and challenge your ground truth and your metrics periodically. Challenge the domain expert to validate and revisit your ground truth dataset whenever you detect misalignments like this.</p>","tags":["LLM","AB Testing","User Preferences","Evaluation Metrics","AI"]},{"location":"writing/2025/04/30/llms-as-judges-notes/","title":"LLMs as Judges: Why Automated Metrics Aren't Enough","text":"<p>LLMs as Judges are just tools, not absolute truths. In the context of generative AI systems, evals are sometimes misunderstood. Some people think that adding another framework, or LLM-as-judge metric will solve the problems and save the day.</p> <p>I don't really care how high your \"Factuality\" or \"Correctness\" metrics are if users don't like the answers your system generates. If these metrics don't correlate with user preference, you have a bigger problem to solve.</p> <p>LLM-as-judge are simply tools pointing to where we should look deeper.</p> <p>Automated metrics help us identify weak spots in our systems that deserve human attention. That's it.</p> <p>No fancy evaluation framework will magically solve your product problems.</p> <p>What matters is the process. Build one that helps you monitor, annotate, measure what real users actually like and iterate. This process will be different in each organisation. How you sample the data to give to domain experts to validate and what \"a correct answer\" actually means in your case will depend on your context and internal processes. </p> <p>Remember, Evals aren\u2019t static datasets or metrics. </p> <p>They\u2019re a living process that enables you to apply the scientific method. </p> <p>Observe, annotate, create hypothesis, design experiments, measure, repeat.</p>","tags":["LLMs","AI Evaluation","User Preference","Automated Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/","title":"My Journey with Evaluation-Driven Development","text":"<p>Over the past year, I've been working on building generative AI systems that actually solve real problems. What I've learned is that the biggest challenge isn't getting an AI to generate responses\u2014it's ensuring those responses are reliable, useful, and continuously improving based on real user feedback.</p> <p>This led me to develop what I call an \"evaluation-driven development\" approach. Instead of building first and evaluating later, I've structured my entire workflow around feedback and evaluation from day one. Here's what I've discovered works.</p> <p></p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#the-framework-i-use","title":"The Framework I Use","text":"<p>My evaluation-driven development process consists of seven interconnected phases that I've refined through multiple projects:</p> <p>Knowledge Base Understanding: I start by identifying requirements and assessing data (taxonomy, metadata) to understand the knowledge base segments I'm working with.</p> <p>Foundation Building: Next, I establish a baseline RAG (Retrieval-Augmented Generation) setup as my technical foundation.</p> <p>Enhanced Processing: I implement optimized data processing, search enhancements, response generation, and prompt engineering.</p> <p>Observability: I deploy comprehensive logging, dashboards, monitoring, and user feedback systems to gain visibility into performance.</p> <p>Review Evaluation: My multi-level evaluation includes unit tests, human + model evaluation, and A/B testing to measure effectiveness.</p> <p>Optimization: Based on feedback and prioritization, I analyze data, improve metadata (particularly domain-specific), and review agent architecture.</p> <p>Iterative Improvement: Finally, I implement deployment and monitoring improvements, trace analysis and curation, and integrate data updates.</p> <p></p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#who-i-collaborate-with","title":"Who I Collaborate With","text":"<p>This process has taught me the importance of cross-functional collaboration. I work closely with:</p> <ul> <li>Product Managers and UX Engineers who focus on interface design</li> <li>Domain Experts who provide specialized knowledge  </li> <li>Data Engineers who manage infrastructure</li> <li>Client Data Engineers who bridge domain and technical needs</li> <li>Beta testers and early adopters who provide real-world validation</li> </ul>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#my-core-goals","title":"My Core Goals","text":"<p>Through this work, I've focused on four key objectives:</p> <ol> <li>Create simple, intuitive UIs with clear feedback visualization</li> <li>Aggregate and prioritize user feedback for targeted iterations</li> <li>Understand knowledge base segments for comprehensive coverage</li> <li>Establish reliable measurement baselines for ongoing improvements</li> </ol> <p>This systematic approach has transformed my standard observability practices into actionable insights, ensuring my solutions evolve in response to real user needs and quantifiable metrics.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#phase-1-understanding-the-knowledge-base","title":"Phase 1: Understanding the Knowledge Base","text":"<p>Goal: Understand KB Segments</p> <p>This initial phase has become the foundation of my approach. I've learned that comprehensive knowledge base analysis before any implementation is crucial for success.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#what-i-focus-on","title":"What I Focus On:","text":"<p>Discovery &amp; Requirements - Identifying the specific knowledge domains needed for the solution - Mapping user query patterns to determine information needs - Documenting domain-specific terminology and concepts</p> <p>Data Assessment - Taxonomy development to categorize information hierarchically - Metadata creation to enhance searchability and relationships - Segmentation of knowledge into functional areas</p> <p>Smart Data Ingestion - Building custom pipelines that ingest domain-specific data sources - Creating intelligent metadata automatically - Preparing baseline content (v0)</p> <p>I've found that this foundational phase ensures I have properly organized and understood the knowledge segments before proceeding to build the RAG baseline. By thoroughly analyzing the knowledge requirements upfront, I create a more effective foundation for all subsequent phases.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#phase-2-foundation-building-rag-baseline-setup","title":"Phase 2: Foundation Building - RAG Baseline Setup","text":"<p>Goal: Setup RAG Baseline</p> <p>This second phase creates the technical foundation upon which all my subsequent enhancements are built.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#key-components-i-implement","title":"Key Components I Implement:","text":"<p>Synthetic Testing Framework - Utilizing RAGAs and DeepEvals for standardized evaluation - Implementing synthetic data validation with Argilla or custom UI - Validating test questions with domain experts to ensure relevance</p> <p>Streamlined Testing Approach - Implementing simple testing using pytest - Focusing on basic result generation rather than complex metrics initially - Using straightforward logging and response analysis techniques</p> <p>Experimentation Tracking Infrastructure - Setting up MLFlow for systematic experiment management - Establishing capabilities to track experiment history - Creating version control for all test configurations</p> <p>Automated Evaluation with LLMs - Implementing LLM-as-a-Judge evaluation methodology - Beginning with standard RAGAS metrics as baseline measurements - Building capabilities to generate evaluation reports and insights</p> <p>This phase prioritizes creating a solid, measurable foundation rather than advanced features. By establishing clear baselines and testing methodologies, I create a framework that allows for data-driven improvements in subsequent phases.</p> <p>Important Note: As I've learned from experience and written about in my post on LLM-as-Judge metrics, automated metrics are just tools\u2014not absolute truths. The real test is whether these metrics correlate with actual user preferences.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#phase-3-observability-and-user-feedback-analysis","title":"Phase 3: Observability and User Feedback Analysis","text":"<p>Goal: Have a Simple UI and an Overview of User Feedback and Evaluations</p> <p>This phase focuses on creating accessible interfaces for both users and developers while implementing robust feedback collection and analysis systems.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#what-i-build","title":"What I Build:","text":"<p>Basic Chat Interface - I use Streamlit for rapid development and deployment - Providing an intuitive, straightforward user experience - Ensuring accessibility for all stakeholders, including non-technical users</p> <p>Integration with Trace Tools - Implementing connections to observability platforms like LangSmith, LangFuse, and MLFlow - Enabling developers to follow the execution path of queries</p> <p>Simple User Feedback Mechanisms - Implementing thumbs-up/thumbs-down ratings integrated directly in the UI - Creating seamless integrations with Slack, tracing tools, and databases - Ensuring high participation rates for continuous feedback collection</p> <p>Feedback Aggregation Dashboard - Developing dashboards for aggregated user feedback - Creating visualizations of feedback patterns and trends - Providing metrics on positive vs. negative feedback ratios</p> <p></p> <p>The observability infrastructure I design is lightweight yet comprehensive, capturing both system performance data and user sentiment. By integrating feedback collection directly into the user interface, I ensure high participation rates and create a continuous stream of evaluation data.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#phase-4-beyond-observability-generating-actionable-insights","title":"Phase 4: Beyond Observability - Generating Actionable Insights","text":"<p>Goal: Aggregate and prioritize user feedback and iterate</p> <p>This phase is where I transform raw observability data into actionable insights through sophisticated analysis and clustering techniques.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#my-analysis-framework","title":"My Analysis Framework:","text":"<p>Data Collection Integration - Setting up ingestion pipelines from observability tools - Collecting both structured feedback and unstructured comments - Capturing user queries with corresponding timestamps and metadata</p> <p>Analysis Implementation - Developing clustering algorithms to group similar feedback patterns - Creating categorization systems for user queries by domain and intent - Implementing automated analysis of response quality metrics</p> <p>Insight Generation Systems - Building dashboards for visualization of feedback trends and patterns - Developing recommendation engines to prioritize system improvements - Establishing automated reporting with actionable next steps</p> <p>This phase transforms my observability from passive monitoring into a strategic development driver by identifying exactly where improvements will have the greatest impact on user satisfaction and system performance.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#phase-5-ab-testing-evaluation","title":"Phase 5: A/B Testing &amp; Evaluation","text":"<p>Goal: Validate improvements through controlled experiments</p> <p>This phase implements controlled experiments to validate improvements and ensure changes positively impact user experience before release.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#my-ab-testing-framework","title":"My A/B Testing Framework:","text":"<p>Controlled Experimentation - Developing capability to serve multiple response versions simultaneously - Implementing variant assignment methodology for unbiased testing - Creating monitoring systems to track performance differences</p> <p>Correlation Analysis - Establishing systems to monitor correlation between user preferences and LLM-as-a-judge metrics - Validating that automated evaluation metrics align with actual user satisfaction - Identifying discrepancies that require metric adjustment</p> <p>This correlation analysis is crucial\u2014I've experienced firsthand how LLM-as-judge metrics can diverge from user preferences. In one project, our automated metrics showed only 55% correctness while users preferred our system over 70% of the time. The lesson: always validate your metrics against real user feedback.</p> <p>Release Confidence Assessment - Defining threshold criteria for determining release readiness - Implementing statistical significance testing for experimental results - Creating dashboards for visualizing confidence intervals and performance differences</p> <p>This phase provides scientific validation of my improvements through controlled experimentation, ensuring that development decisions are based on empirical evidence rather than assumptions.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#the-three-level-evaluation-system-i-use","title":"The Three-Level Evaluation System I Use","text":"<p>I've developed a comprehensive, multi-level approach that ensures continuous improvement through systematic testing:</p> <p>Level 1: Unit Tests - Goal: Quickly catch obvious issues with minimal resources - Process: Fast, automated tests that run on every code change - Characteristics: Fastest and cheapest, focus on basic assertions, avoid actual LLM calls</p> <p>Level 2: Human &amp; Model Evaluation - Goal: Identify subtle issues and potential improvements - Process: Combined human review with LLM-as-a-Judge evaluation - Characteristics: Log and analyze conversation traces, combines human expertise with automated metrics</p> <p>Level 3: A/B Testing - Goal: Validate user value and business outcomes - Process: Live testing with real users and statistical analysis - Characteristics: Tests with actual users in real scenarios, measures impact on business metrics</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#the-continuous-improvement-loop","title":"The Continuous Improvement Loop","text":"<p>The entire process I've developed forms a continuous improvement cycle:</p> <ol> <li>Observability provides raw data and feedback</li> <li>Review Evaluation analyzes this data at multiple levels</li> <li>Optimization translates insights into targeted improvements</li> <li>Iterative Improvement implements changes and updates</li> </ol>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#key-lessons-learned","title":"Key Lessons Learned","text":"<p>Through this journey, I've discovered several critical success factors:</p> <ul> <li>Commitment to data-driven decision making - Every improvement needs to be validated</li> <li>Integration of both automated and human evaluation - Neither alone is sufficient</li> <li>Clear prioritization based on quantifiable metrics - Focus efforts where they'll have the most impact</li> <li>Continuous experimentation and validation - Build learning into every release</li> <li>Cross-functional collaboration - Great AI systems require diverse expertise</li> </ul>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#sample-size-matters-for-statistical-confidence","title":"Sample Size Matters for Statistical Confidence","text":"<p>One practical challenge I've encountered is determining how many samples we need for reliable evaluation. As I discuss in my post about evaluation dataset sample sizes, statistical significance matters immensely. For 95% confidence with \u00b15% margin of error, you need around 385 samples. This becomes a resource planning question: how much time can you borrow from domain experts for annotation?</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#the-enterprise-reality","title":"The Enterprise Reality","text":"<p>The broader context here is what I've observed about enterprise AI adoption: the real bottleneck isn't model capability\u2014it's domain-specific evaluation. Most organizations are still between Level 1 and Level 2 AI maturity, using AI as a productivity boost rather than as a true system backbone. Without rigorous evaluation, enterprise trust is impossible.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#looking-forward","title":"Looking Forward","text":"<p>This evaluation-driven approach has fundamentally changed how I build AI systems. Instead of hoping my solutions work well, I now have systematic ways to measure, understand, and improve them based on real user feedback and rigorous testing.</p> <p>The framework isn't just about building better AI\u2014it's about building AI that gets better over time. And in a field that's evolving as rapidly as generative AI, that continuous improvement capability might be the most valuable feature of all.</p> <p>If you're building AI systems and struggling with evaluation, I'd encourage you to start with even simple feedback collection. The insights you'll gain from real users interacting with your system will be more valuable than any synthetic benchmark.</p> <p>As I've written about in my thoughts on context engineering and evaluation, we're moving beyond prompt engineering to become \"context engineers.\" With solutions like Anthropic's Model Context Protocol standardizing data plumbing, the real differentiator for AI engineers will be building robust evaluation workflows and feedback loops.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/2025/05/03/my-evaluation-driven-development-journey/#related-posts","title":"Related Posts","text":"<p>If you found this post helpful, you might also be interested in:</p> <ul> <li>[My Journey with Evaluation-Driven Development][](notes-on-evaluation-driven-development.md) - My Journey with Evaluation-Driven Development</li> <li>When LLM-as-Judge Metrics and User Preferences Diverge - Real examples of how automated metrics can mislead</li> <li>LLMs as Judges: Why Automated Metrics Aren't Enough - My thoughts on the limitations of LLM-as-judge approaches</li> <li>The Real Bottleneck in Enterprise AI Adoption - Why evaluation, not models, is the limiting factor</li> <li>How Many Samples Do We Need in Our Evaluation Dataset? - Practical guidance on statistical confidence and sample sizes</li> <li>From Prompt to Context Engineering - Why evaluation is becoming the key differentiator</li> </ul> <p>What's your experience been with AI evaluation? I'd love to hear about the approaches you've tried and what's worked (or hasn't worked) for your use cases.</p>","tags":["AI Development","User Feedback","Continuous Improvement","Evaluation Metrics","Generative AI"]},{"location":"writing/archive/2025/","title":"2025","text":""},{"location":"writing/category/applied-ai/","title":"Applied AI","text":""},{"location":"writing/category/ai-evaluation/","title":"AI Evaluation","text":""}]}