---
categories:
- Applied AI
comments: true
description: Explore insights on LLMs, evaluation strategies, and AI adoption. Join
  the conversation on consulting and personal work in AI.
tags:
- LLMs
- AI evaluation
- consulting
- personal growth
- RAG
---

# Writing

I write about a mix of consulting, personal work, and applying llms.

[:material-linkedin: Follow me on LinkedIn](https://linkedin.com/in/arianpasquali){.md-button .md-button--secondary}  

For posts about RAG (Retrieval-Augmented Generation) or LLMs (Large Language Models), check out the category labels in the sidebar. Here are some of my best posts on these topics:


## Insights

- [When LLM-as-Judge Metrics and User Preferences Diverge](posts/llm-as-judge-and-user-preference-correlation.md) - My journey developing an evaluation-driven approach for generative AI systems, sharing lessons learned about structured feedback, multi-level evaluation, and building more reliable AI applications.
- [LLMs as Judges: Why Automated Metrics Aren't Enough](posts/llms-as-judges-notes.md) - My thoughts on the limitations of LLM-as-judge approaches
- [The Real Bottleneck in Enterprise AI Adoption](posts/ai-adoption-bottleneck.md) - Why evaluation, not models, is the limiting factor
- [How Many Samples Do We Need in Our Evaluation Dataset?](posts/dataset-samples.md) - Practical guidance on statistical confidence and sample sizes
- [From Prompt to Context Engineering](posts/generative-ai-protocols-and-tartare.md) - Why evaluation is becoming the key differentiator