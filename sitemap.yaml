404.md:
  hash: 104342d91d9a9a4f75205a7e2c04599e
  summary: 'This is a "404 - Page Not Found" error message indicating the page you
    are looking for does not exist. Possible reasons include the page being moved
    or deleted, a broken link, or a typo in the URL. Users are advised to return to
    the homepage or use the search function. Key points include: "404 error," "page
    not found," "broken link," "URL typo," and "homepage navigation."'
index.md:
  hash: 8611c082c90c02276a624aaea0cf80b8
  summary: 'This page introduces a machine learning engineer at Faktion, specializing
    in Generative AI evaluation methodologies, with a focus on LLM evaluation pipelines
    and the development of structured evaluation processes to enhance AI performance
    and user satisfaction. Key areas of expertise include evaluation-driven development,
    enterprise AI adoption challenges, and the creation of systematic approaches to
    bridge automated metrics and user satisfaction. The engineer is also a creator
    of YAKE, a keyword extraction library, and actively contributes to academic conferences
    and open-source projects. Opportunities for collaboration and discussions on AI
    challenges and methodologies are encouraged. Keywords: Generative AI, LLM evaluation,
    AI performance, user satisfaction, evaluation-driven development, enterprise AI,
    YAKE, open-source contributions, academic involvement.'
writing/index.md:
  hash: d0e6fa4f882ad5eddb73f4a4db6c0bc3
  summary: This site explores insights and expert tips on consulting, AI applications,
    and writing, with a focus on topics such as Retrieval-Augmented Generation (RAG)
    and Large Language Models (LLMs). Key articles include discussions on the divergence
    between LLM-as-Judge metrics and user preferences, highlighting the importance
    of structured feedback and multi-level evaluation for reliable AI applications.
    The limitations of automated metrics for LLMs, the evaluation bottleneck in enterprise
    AI adoption, and guidance on sample sizes for statistical confidence in evaluations
    are also covered. The content emphasizes the shift from prompt engineering to
    context engineering as a critical differentiator in generative AI. Keywords include
    AI engineering, consulting, evaluation, RAG, LLMs, AI adoption, and context engineering.
writing/posts/ai-adoption-bottleneck.md:
  hash: 2ae425b86a248e8b5af023b7e22cdb36
  summary: 'The article "The Real Bottleneck in Enterprise AI Adoption: Evaluation,
    Not Models" by arianpasquali explores why domain-specific evaluation, rather than
    model capability, is the critical obstacle to enterprise AI adoption. It argues
    that public benchmarks fail to address real-world business needs and emphasizes
    the necessity for custom evaluation and expert validation to build trust in AI
    systems. The content outlines the importance of a structured, step-by-step approach
    to operational AI, highlighting the need for reliable infrastructure over quick,
    superficial solutions. Key levels of AI adoption include integrating generative
    AI with proprietary data, contextual AI for knowledge integration, AI agents for
    business tasks, and multi-agent workflow orchestration. The article asserts that
    understanding these levels and developing necessary skills and organizational
    needs is essential for successfully operationalizing AI at scale. Key terms include
    "domain-specific evaluation," "enterprise AI," "trust in AI," "AI adoption stack,"
    and "operational AI."'
writing/posts/dataset-samples.md:
  hash: 4f3f9dd41f03065eb8f47eb7c7816f08
  summary: 'This practical guide explores determining the optimal sample size for
    evaluation datasets in generative AI projects, balancing statistical confidence
    with the costs of annotation. Key considerations include the confidence level
    (90%, 95%, 99%), acceptable margin of error, and expected data variance. The article
    provides examples of sample sizes needed for various confidence levels and emphasizes
    the importance of statistical significance in AI system evaluations. It highlights
    the need to involve costly domain experts efficiently and suggests quantifying
    the trade-off between annotation time and desired confidence levels. Keywords:
    generative AI, evaluation dataset, sample size, statistical confidence, annotation
    costs, domain experts.'
writing/posts/generative-ai-and-digital-humanities.md:
  hash: 5b2bfa14984a55e72b11aa4c5ad418db
  summary: 'This article highlights the crucial role of experts in digital humanities,
    anthropology, and qualitative research in evaluating and shaping generative AI
    systems. While the tech industry focuses on "prompt engineering," it is the qualitative
    researchers who excel in crafting nuanced questions, understanding context, and
    analyzing complex human interactions. Their skills in context analysis, iterative
    inquiry, and critical evaluation are essential for designing, evaluating, and
    aligning AI models. The article argues that these experts are well-equipped to
    lead the conversation on responsible AI evaluation and human-centered AI systems,
    emphasizing that the future of AI is more human-centered, aligning perfectly with
    their expertise. Keywords: generative AI, digital humanities, qualitative research,
    AI evaluation, human-centered AI, context analysis, responsible AI.'
writing/posts/generative-ai-protocols-and-tartare.md:
  hash: 148901588fd0b659ec70fa81fd78757b
  summary: 'The article "From Prompt to Context Engineering: Why Evaluation is the
    Real AI Differentiator" discusses the evolving field of generative AI, emphasizing
    the shift from traditional prompt engineering to "context engineering." As AI
    technology matures, standardized protocols like Anthropic''s Model Context Protocol
    (MCP) are streamlining context management. The author argues that the future of
    AI lies in developing domain-specific evaluation workflows to ensure system accuracy
    and reliability, particularly in enterprise settings. By focusing less on data
    plumbing and more on evaluation and feedback loops, AI engineers can enhance system
    performance. The article highlights the importance of robust confidence assessments
    and the need to allocate resources toward building effective evaluation pipelines.
    Key terms include Generative AI, context engineering, Model Context Protocol (MCP),
    and domain-specific evaluation.'
writing/posts/llm-as-judge-and-user-preference-correlation.md:
  hash: 14c1467efa5a6b4b2c90cea03b5c0fef
  summary: 'This article discusses the potential pitfalls of relying solely on automated
    LLM-as-judge metrics for evaluating generative AI systems, highlighting their
    potential divergence from real user preferences. A case study is presented where
    an initially used metric penalized informative LLM-generated answers, leading
    to a misalignment with user satisfaction. By revising the evaluation metric to
    better align with user preferences, factual accuracy increased to 78%. Key takeaways
    include the importance of calibrating metrics with AB testing, focusing on metrics
    that predict user satisfaction, and periodically revisiting ground truths with
    domain experts. Keywords: automated evaluation metrics, user preferences, LLM-as-a-Judge,
    AB testing, generative AI, factual accuracy, user satisfaction.'
writing/posts/llms-as-judges-notes.md:
  hash: e2765190f4eb87e57b36e91b085f447a
  summary: "The article \"LLMs as Judges: Why Automated Metrics Aren't Enough\" by\
    \ arianpasquali highlights the limitations of using Large Language Models (LLMs)\
    \ as standalone evaluators in generative AI systems. It emphasizes that while\
    \ automated metrics like \"Factuality\" and \"Correctness\" can identify weaknesses,\
    \ they should not replace human-centered evaluation processes. The core objective\
    \ is to encourage organizations to develop dynamic evaluation frameworks that\
    \ focus on user preferences and incorporate human validation. Key concepts include\
    \ the importance of iterative processes\u2014observing, annotating, hypothesizing,\
    \ experimenting, and measuring\u2014tailored to specific organizational contexts\
    \ for improving AI system outputs."
writing/posts/notes-on-evaluation-driven-development.md:
  hash: f9116087543a3b8d7d5acbddc1c9f9c8
  summary: 'In the blog post "My Journey with Evaluation-Driven Development," the
    author shares insights from developing an evaluation-driven approach for generative
    AI systems. The focus is on ensuring AI-generated responses are reliable and useful
    through a structured feedback and multi-level evaluation process from the start.
    The seven-phase framework emphasizes knowledge base understanding, foundation
    setup, enhanced processing, observability, and iterative improvement, supported
    by cross-functional collaboration with domain experts and beta testers. Key takeaways
    include integrating both automated and human evaluations to inform data-driven
    decisions and establishing feedback loops for continuous improvement, vital for
    enterprise AI adoption. Core keywords: evaluation-driven development, generative
    AI systems, feedback, multi-level evaluation, cross-functional collaboration,
    continuous improvement.'
